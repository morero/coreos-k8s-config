#cloud-config

hostname: "MYHOSTNAME"
write_files:
  - path: /etc/modprobe.d/bonding.conf
    content: |
      # Prevent kernel from automatically creating bond0 when the module is loaded.
      # This allows systemd-networkd to create and apply options to bond0.
      options bonding max_bonds=0
  - path: /etc/systemd/network/10-e.network
    permissions: 0644
    owner: root
    content: |
      [Match]
      Name=e*

      [Network]
      Bond=bond0
  - path: /etc/systemd/network/20-bond.netdev
    permissions: 0644
    owner: root
    content: |
      [NetDev]
      Name=bond0
      Kind=bond

      [Bond]
      Mode=0 # defaults to balance-rr
      MIIMonitorSec=100
  - path: /etc/udev/rules.d/50-rbd.rules
    content: |
      KERNEL=="rbd[0-9]*", ENV{DEVTYPE}=="disk", PROGRAM="/opt/bin/ceph-rbdnamer %k", SYMLINK+="rbd/%c{1}/%c{2}"
      KERNEL=="rbd[0-9]*", ENV{DEVTYPE}=="partition", PROGRAM="/opt/bin/ceph-rbdnamer %k", SYMLINK+="rbd/%c{1}/%c{2}-part%n"
  - path: /opt/bin/ceph
    permissions: "0755"
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm -v /etc/ceph:/etc/ceph ceph/base ceph "$@"
  - path: /opt/bin/ceph-disk
    permissions: "0755"
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm --privileged=true -v /etc/ceph:/etc/ceph -v /dev:/dev ceph/base ceph-disk "$@"
  - path: /opt/bin/ceph-rbdnamer
    permissions: "0755"
    content: |
      #!/bin/sh
      DEV=$1
      NUM=`echo $DEV | sed 's#p.*##g' | tr -d 'a-z'`
      POOL=`cat /sys/devices/rbd/$NUM/pool`
      IMAGE=`cat /sys/devices/rbd/$NUM/name`
      SNAP=`cat /sys/devices/rbd/$NUM/current_snap`
      if [ "$SNAP" = "-" ]; then
          echo -n "$POOL $IMAGE"
      else
          echo -n "$POOL $IMAGE@$SNAP"
      fi
  - path: /opt/bin/rados
    permissions: "0755"
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm -v /etc/ceph:/etc/ceph ceph/base rados "$@"
  - path: /opt/bin/rbd
    permissions: "0755"
    content: |
      #!/bin/sh
      /usr/bin/docker run --rm -v /etc/ceph:/etc/ceph -v /sys:/sys --net=host --privileged=true ceph/base rbd "$@"
  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:HYPERKUBE_VERSION
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers=MYETCDENDPOINTS
          - --allow-privileged=true
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          - --advertise-address=MYIPADDRESS
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:HYPERKUBE_VERSION
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          - --proxy-mode=iptables
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:HYPERKUBE_VERSION
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 1
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:HYPERKUBE_VERSION
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 1
  - path: /etc/kubernetes/manifests/policy-controller.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: calico-policy-controller
        namespace: calico-system
      spec:
        hostNetwork: true
        containers:
          # The Calico policy controller.
          - name: k8s-policy-controller
            image: calico/kube-policy-controller:v0.2.0
            env:
              - name: ETCD_ENDPOINTS
                value: "http://MYIPADDRESS:2379"
              - name: K8S_API
                value: "http://127.0.0.1:8080"
              - name: LEADER_ELECTION
                value: "true"
          # Leader election container used by the policy controller.
          - name: leader-elector
            image: quay.io/calico/leader-elector:v0.1.0
            imagePullPolicy: IfNotPresent
            args:
              - "--election=calico-policy-election"
              - "--election-namespace=calico-system"
              - "--http=127.0.0.1:4040"
  - path: /etc/kubernetes/cni/net.d/10-calico.conf
    content: |
      {
        "name": "calico",
        "type": "flannel",
        "delegate": {
          "type": "calico",
          "etcd_endpoints": "MYETCDENDPOINTS",
          "log_level": "none",
          "log_level_stderr": "info",
          "hostname": "MYIPADDRESS",
          "policy": {
            "type": "k8s",
            "k8s_api_root": "http://127.0.0.1:8080/api/v1/"
          }
        }
      }
  - path: /etc/ssh/sshd_config
    permissions: 0600
    owner: root:root
    content: |
      # Use most defaults for sshd configuration.
      UsePrivilegeSeparation sandbox
      Subsystem sftp internal-sftp
      ClientAliveInterval 180
      UseDNS no
      UsePAM yes
      PrintMotd no # handled by PAM

      PermitRootLogin no
      AllowUsers core
      PasswordAuthentication no
      ChallengeResponseAuthentication no
coreos:
  flannel:
    etcd_endpoints: "MYETCDENDPOINTS"
    interface: "MYIPADDRESS"
  etcd2:
    name: "MYHOSTNAME"
    advertise-client-urls: "http://MYIPADDRESS:2379"
    initial-advertise-peer-urls: "http://MYIPADDRESS:2380"
    initial-cluster-token: "etcd-cluster-1"
    listen-client-urls: "http://0.0.0.0:2379,http://0.0.0.0:4001"
    listen-peer-urls: "http://0.0.0.0:2380"
  update:
    reboot-strategy: "etcd-lock"
  units:
    - name: systemd-timesyncd.service
      command: start
      enable: true
    - name: ntpd.service
      command: stop
      mask: true
    - name: etcd2.service
      command: start
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
    - name: kubelet.service
      command: start
      enable: true
      content: |
        [Service]
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests

        Environment=KUBELET_VERSION=HYPERKUBE_VERSION
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --api-servers=http://127.0.0.1:8080 \
          --network-plugin-dir=/etc/kubernetes/cni/net.d \
          --network-plugin=cni \
          --register-schedulable=false \
          --allow-privileged=true \
          --config=/etc/kubernetes/manifests \
          --hostname-override=MYIPADDRESS \
          --cluster-dns=10.3.0.10 \
          --cluster-domain=cluster.local
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
    - name: calico-node.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Calico per-host agent
        Requires=network-online.target
        After=network-online.target

        [Service]
        Slice=machine.slice
        Environment=CALICO_DISABLE_FILE_LOGGING=true
        Environment=HOSTNAME=MYIPADDRESS
        Environment=IP=MYIPADDRESS
        Environment=FELIX_FELIXHOSTNAME=MYIPADDRESS
        Environment=CALICO_NETWORKING=false
        Environment=NO_DEFAULT_POOLS=true
        Environment=ETCD_ENDPOINTS=http://MYIPADDRESS:2379
        ExecStart=/usr/bin/rkt run --inherit-env --stage1-from-dir=stage1-fly.aci \
        --volume=modules,kind=host,source=/lib/modules,readOnly=false \
        --mount=volume=modules,target=/lib/modules \
        --trust-keys-from-https quay.io/calico/node:v0.22.0

        KillMode=mixed
        Restart=always
        TimeoutStartSec=0

        [Install]
        WantedBy=multi-user.target
    - name: data.mount
      command: start
      content: |
        [Mount]
        What=/dev/sdb
        Where=/data
        Type=btrfs
ssh_authorized_keys:
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDAaWHESLrzhMX4MmMwc/SmUtH35dDvvbS4XaQwGQUKR/SjSDaHaJS5ErWY2/Uk/xjDykyTFUlIVD6mcueHmHEhmB0YDNN3nsmA6VIht4qr6Y17u8PPAvt72QUaHnq6G7WmLeVH5FKLl1tAKnZvpIPGdwFNWNmuBxr1TPjpnKqnxFT4qkX96YFw80ZxMlqqgpUgwn/RRt7/jknAM1BtyG253xWoklaqQ9n4gYeaAuPulx3rln6aeTeXCQbjJFcU64LtX9nAX2aXffP1KqY7z7x8T1tcVwmBlsabB0vWEqF9aRvWRTkvSzB6XuSYYJSxvA9+JsvV4dRod38NfOHY9a3J
